{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88facfc2-7a17-4a44-8b34-29761d350b72",
   "metadata": {},
   "source": [
    "# Deploy MusicGen Large model on Amazon SageMaker for Asynchronous Inferencing\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Transcription_on_SM_endpoint.ipynb\n",
    "https://huggingface.co/docs/transformers/model_doc/musicgen#generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5dd1f7-755c-40e6-9628-e80b97c8130f",
   "metadata": {},
   "source": [
    "Install necessary packages to run this notebook on SageMaker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1f938-35f2-4eee-a319-ac2e199ff29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq pip\n",
    "!pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af52d69-a712-46d2-8d32-688d70b72fa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Inference Scripts\n",
    "\n",
    "We will create model directory that holds the code artefacts such as inference python script and the requirements.txt that holds all the relevant python packages that will be installed when the model is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104af8c1-e7ef-42e2-b419-28a899800f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir model\n",
    "!mkdir model/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2f848-b6fa-4320-8e2f-470e04696271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## requirements.txt https://github.com/facebookresearch/audiocraft/blob/main/README.md\n",
    "'''\n",
    "# Best to make sure you have torch installed first, in particular before installing xformers.\n",
    "# Don't run this if you already have PyTorch installed.\n",
    "pip install 'torch>=2.0'\n",
    "# Then proceed to one of the following\n",
    "pip install -U audiocraft  # stable release\n",
    "pip install -U git+https://git@github.com/facebookresearch/audiocraft#egg=audiocraft  # bleeding edge\n",
    "pip install -e .  # or if you cloned the repo locally (mandatory if you want to train).\n",
    "'''\n",
    "with open(\"model/code/requirements.txt\", \"w\") as f:\n",
    "    f.write(\"transformers==4.34.1\\n\")\n",
    "    f.write(\"boto3\\n\")\n",
    "    f.write(\"torch>=2.0\\n\")\n",
    "    f.write(\"scipy\\n\")\n",
    "    f.write(\"uuid\\n\")\n",
    "    f.write(\"audiocraft\\n\")\n",
    "    f.write(\"ffmpeg\\n\")\n",
    "    f.write(\"ffmpeg-python\\n\")\n",
    "    f.write(\"git+https://git@github.com/facebookresearch/audiocraft#egg=audiocraft\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449cccb-5535-4bcc-af1f-9b9ff057ca8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model/code/inference.py\n",
    "\n",
    "import boto3\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "import scipy\n",
    "import uuid\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-large\")\n",
    "    device = \"cuda:0,1,2,3\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def _process_input(data):\n",
    "    \n",
    "    #defaults\n",
    "    default_config = { 'guidance_scale': 3, 'max_new_tokens': 256, 'do_sample': True }\n",
    "    default_texts = [\"Morning sunshine, beats, ukelele, happy swings\"]\n",
    "    \n",
    "    # obtain input data\n",
    "    texts = data.get('texts', default_texts)\n",
    "    config = data.get('config', default_config)\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(\"facebook/musicgen-large\")\n",
    "    inputs = processor(\n",
    "        text = texts, #[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    assert type(config) == dict\n",
    "    guidance_scale = config.get('guidance_scale', default_config.get('guidance_scale'))\n",
    "    max_new_tokens = config.get('max_new_tokens', default_config.get('max_new_tokens'))\n",
    "    do_sample = config.get('do_sample', default_config.get('do_sample'))\n",
    "    \n",
    "    processed_config = {'guidance_scale': guidance_scale, 'do_sample': do_sample, 'max_new_tokens': max_new_tokens}\n",
    "    return inputs, processed_config\n",
    "\n",
    "\n",
    "def _upload_to_s3(wav_on_disk, bucket_name):\n",
    "    s3 = boto3.resource('s3')\n",
    "    target_file = wav_on_disk.split('/')[-1]\n",
    "    prefix = 'musicgen_large/output'\n",
    "    key = f'{prefix}/{target_file}'\n",
    "    s3.Bucket(bucket_name).upload_file(wav_on_disk, key)\n",
    "    return f\"s3://{bucket_name}/{key}\"\n",
    "\n",
    "\n",
    "def _upload_wav_files(wav_files, bucket_name):\n",
    "    #wav_files = [f for f in os.listdir('.') if f.endswith('.wav')]\n",
    "    futures_list = []\n",
    "    results = []\n",
    "    max_workers = multiprocessing.cpu_count () * 2 + 1\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor: # I/O bound process\n",
    "        for file in wav_files:\n",
    "            futures = executor.submit(_upload_to_s3, file, bucket_name)\n",
    "            futures_list.append(futures)\n",
    "\n",
    "        for future in futures_list:\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                results.append(result)\n",
    "            except Exception:\n",
    "                results.append(None)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _write_wavs_to_disk(sampling_rate, audio_values):\n",
    "    r = len(audio_values)\n",
    "    prefix = str(uuid.uuid1())\n",
    "    disk_wav_locations = []\n",
    "    for i in range(r):\n",
    "        # Write wav to Disk\n",
    "        wav_file = f\"{prefix}_{i}_musicgen_large_out.wav\"\n",
    "        wav_on_disk = f'/tmp/{wav_file}'\n",
    "        scipy.io.wavfile.write(wav_on_disk, rate=sampling_rate, data=audio_values[i, 0].cpu().numpy())\n",
    "        disk_wav_locations.append(wav_on_disk)\n",
    "    return disk_wav_locations\n",
    "\n",
    "\n",
    "def _delete_file_on_disk(filename):\n",
    "    if os.path.isfile(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "\n",
    "def predict_fn(data, model):\n",
    "    \n",
    "    # https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor.predict\n",
    "    \n",
    "    bucket_name = data.pop('bucket_name', None)\n",
    "    \n",
    "    if not bucket_name:\n",
    "        raise ValueError(\"bucket_name is required ex: sagemaker_default_bucket_007\")\n",
    "        \n",
    "    inputs, processed_config = _process_input(data)\n",
    "    \n",
    "    device = \"cuda:0,1,2,3\" if torch.cuda.is_available() else \"cpu\"\n",
    "    audio_values = model.generate(**inputs.to(device), \n",
    "                                  max_new_tokens=processed_config.pop('max_new_tokens'), \n",
    "                                  guidance_scale = processed_config.pop('guidance_scale'),\n",
    "                                  do_sample = processed_config.pop('do_sample'))\n",
    "\n",
    "    # process generated output audio_values\n",
    "    sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "    disk_wav_locations = _write_wavs_to_disk(sampling_rate, audio_values)\n",
    "\n",
    "    # Upload wavs to S3\n",
    "    results = _upload_wav_files(disk_wav_locations, bucket_name)\n",
    "    # Clean up disk\n",
    "    for wav_on_disk in disk_wav_locations:\n",
    "        _delete_file_on_disk(wav_on_disk)\n",
    "    return {\n",
    "        \"generated_outputs_s3\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d35645-e796-40f3-bfe5-8dc8378da653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c025c-9a98-442f-866a-7927a980494b",
   "metadata": {},
   "source": [
    "## Place the inference scripts archive on Amazon S3\n",
    "\n",
    "We will create the archive of the inference scripts and upload those to Amazon S3 bucket. The uploaded uri of this object on S3 will later be used to create the HuggingFace Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3098e-1194-4805-8cfd-f33880dd7211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be2a9d-63a6-4938-bbfb-7ab687ad69ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf code/.ipynb_checkpoints*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8ea5b-753f-4ae8-aeff-226fad09eabd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de657b6f-7c22-4496-8c74-05d1661527c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89056fe5-98e8-44a6-b484-f01d83872293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "musicgen_prefix = 'musicgen_large'\n",
    "s3_model_key = f'{musicgen_prefix}/model/model.tar.gz'\n",
    "s3_model_location = f\"s3://{sagemaker_session_bucket}/{s3_model_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c407864-cf11-4c09-9d59-645bfe187bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(sagemaker_session_bucket).upload_file(\"model.tar.gz\", s3_model_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968baaea-6613-4f5e-9469-ccc55ea35ef7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Asynchronous Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "To create the Asynchronous Inference Endpoint, we will perform the following steps:\n",
    "\n",
    "1. Create a model of type HuggingFaceModel since we are using the musicgen large model from HuggingFace as model provider. \n",
    "2. We then create an asynchronous endpoint configuration whose notification configuration is associated to Amazon SNS for success and error topics. \n",
    "3. We will finally deploy the model to generate a SageMaker Endpoint for asynchronous inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376633d-eb8c-46a4-bb55-110db4a72bfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb012660-5957-4ced-b2f3-e1527d99839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "from sagemaker.s3 import s3_path_join\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "async_endpoint_name = name_from_base(\"musicgen-large-v1-asyc\")\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "# AudioCraft's models requires Python 3.9, PyTorch 2.0.0 or latest. (https://github.com/facebookresearch/audiocraft/blob/main/README.md)\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    name=async_endpoint_name,\n",
    "    model_data=s3_model_location,  # path to your model and script\n",
    "    role=role,  # iam role with permissions to create an Endpoint\n",
    "    transformers_version=\"4.28\",  # transformers version used\n",
    "    pytorch_version=\"2.0\",  # pytorch version used\n",
    "    py_version=\"py310\",  # python version used\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064219c4-3796-4527-9643-71214a1ff7eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Asynchronous Inference Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a716ed-cf50-41d5-8914-2cc3664bf339",
   "metadata": {},
   "source": [
    "#### Create Amazon SNS topics for Success and Failure Notification configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7e75a-83d1-40d8-a553-b8d49a855222",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031370d-1e5b-4fa7-8a1b-742eba6726e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# https://stackoverflow.com/a/8015152\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "\n",
    "from utils.sns_client import SnsClient\n",
    "del sys.path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673834a2-92dc-4a9c-9654-97d821757904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SNS topic\n",
    "import time\n",
    "#from utils.sns_client import SnsClient\n",
    "\n",
    "sns_client = SnsClient(boto3.client(\"sns\"))\n",
    "timestamp = time.time_ns()\n",
    "topic_names = [f\"musicgen-large-topic-SuccessTopic-{timestamp}\", f\"musicgen-large-topic-ErrorTopic-{timestamp}\"]\n",
    "\n",
    "topic_arns = []\n",
    "for topic_name in topic_names:\n",
    "    print(f\"Creating topic {topic_name}.\")\n",
    "    response = sns_client.create_topic(topic_name)\n",
    "    topic_arns.append(response.get('TopicArn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98a18b-4227-41fb-8d64-2ba7993dfd0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_arns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86874f07-8b7f-4140-8c76-41729a406f85",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Async Inference Configuration and associate it with SNS topics using notification configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5545b7f-870f-4740-939d-c8ee3f225e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create async endpoint configuration\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=s3_path_join(\n",
    "        \"s3://\", sagemaker_session_bucket, \"musicgen_large/async_inference/music_output\"\n",
    "    ),  # Where our results will be stored\n",
    "    # Add nofitication SNS if needed\n",
    "    notification_config={\n",
    "        \"SuccessTopic\": topic_arns[0],\n",
    "        \"ErrorTopic\": topic_arns[1],\n",
    "    },  #  Notification configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71786423-5a25-4792-8122-465bb3a1840b",
   "metadata": {},
   "source": [
    "### Deploy the model to generate a SageMaker Endpoint for asynchronous inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b625a-c478-4a01-9822-9fed453c8574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy the endpoint\n",
    "async_predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    async_inference_config=async_config,\n",
    "    endpoint_name=async_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dfa36b-ff09-430b-a534-1c78921d4f0d",
   "metadata": {},
   "source": [
    "Meanwhile the model gets deployed, you can refer to the following info about Amazon SageMaker and Facebook Musicgen.\n",
    "\n",
    "- https://huggingface.co/facebook/musicgen-large\n",
    "- https://huggingface.co/docs/transformers/model_doc/musicgen#generation\n",
    "- https://github.com/facebookresearch/audiocraft/blob/main/README.md\n",
    "- https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model\n",
    "- https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor.predict\n",
    "- https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Transcription_on_SM_endpoint.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e1fac-35de-45d9-bd22-6cb3410dbfd2",
   "metadata": {},
   "source": [
    "Let's save the variables that will be re-used in the infer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3be25-db70-4e9c-9054-e3978ecd62b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name=async_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626079df-5dec-4c67-be91-52698f52e567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store \\\n",
    "endpoint_name \\\n",
    "sagemaker_session_bucket \\\n",
    "topic_arns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b74dd-b98d-4eda-80b5-1161eae18844",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb05593-1d54-44d1-843a-0ba0b54e8221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329719a4-84f9-40b7-8dec-43b137ca13c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.r5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
